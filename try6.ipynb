{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "996c334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/keqiaoli/Desktop/RankingSHAP/RankingShap/')\n",
    "import importlib\n",
    "import try6\n",
    "importlib.reload(try6)\n",
    "import try6\n",
    "from try6 import *\n",
    "\n",
    "import try6_wordLevel\n",
    "importlib.reload(try6_wordLevel)\n",
    "import try6_wordLevel\n",
    "from try6_wordLevel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c765911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/keqiaoli/Desktop/RankingSHAP/RankingShap/')\n",
    "import importlib\n",
    "import perturbation_sentenceLevel_transformer\n",
    "importlib.reload(perturbation_sentenceLevel_transformer)\n",
    "from perturbation_sentenceLevel_transformer import *\n",
    "# import try6_wordLevel\n",
    "# from try6_wordLevel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a31156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AutoModelForCausalLM', 'AutoModelForMaskedLM', 'AutoModelForSeq2SeqLM', 'AutoTokenizer', 'BartForConditionalGeneration', 'BartTokenizer', 'Callable', 'Dict', 'Enum', 'GPT2LMHeadModel', 'GPT2Tokenizer', 'List', 'Optional', 'SentenceTransformer', 'T5ForConditionalGeneration', 'T5Tokenizer', 'TransformerIRModelTester', 'TransformerPerturbationType', 'TransformerPerturbator', 'TransformerTestCase', 'Tuple', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'dataclass', 'defaultdict', 'json', 'np', 'pipeline', 're', 'torch', 'util']\n"
     ]
    }
   ],
   "source": [
    "import perturbation_sentenceLevel_transformer\n",
    "print(dir(perturbation_sentenceLevel_transformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4072ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dec1613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing transformer-based tester...\n",
      "Using device: cpu\n",
      "Loading sentence transformer...\n",
      "\n",
      "Generating transformer-based perturbations...\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading t5-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 87\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# # Run tests\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# print(\"\\nRunning sensitivity tests...\")\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# results = tester.run_sensitivity_test(test_cases)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# print(\"\\nResults saved to 'transformer_sensitivity_results.json'\")\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[43mexample_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 30\u001b[0m, in \u001b[0;36mexample_usage\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Generate test cases\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerating transformer-based perturbations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m test_cases \u001b[38;5;241m=\u001b[39m \u001b[43mtester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_test_cases\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperturbation_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTransformerPerturbationType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT5_PARAPHRASE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTransformerPerturbationType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mADVERSARIAL_PARAPHRASE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTransformerPerturbationType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTYLE_TRANSFER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTransformerPerturbationType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQUERY_EXPANSION\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintensity_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43msamples_per_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Multiple samples for better statistics\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_cases)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m test cases\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Show examples\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/RankingSHAP/RankingShap/perturbation_sentenceLevel_transformer.py:733\u001b[0m, in \u001b[0;36mTransformerIRModelTester.generate_test_cases\u001b[0;34m(self, queries, perturbation_types, intensity_levels, samples_per_query)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(samples_per_query):\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 733\u001b[0m         perturbed, details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperturbator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_perturbation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensity\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[1;32m    738\u001b[0m         semantic_sim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperturbator\u001b[38;5;241m.\u001b[39m_calculate_semantic_similarity(\n\u001b[1;32m    739\u001b[0m             query, perturbed\n\u001b[1;32m    740\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/RankingSHAP/RankingShap/perturbation_sentenceLevel_transformer.py:149\u001b[0m, in \u001b[0;36mTransformerPerturbator.apply_perturbation\u001b[0;34m(self, text, perturbation_type, intensity)\u001b[0m\n\u001b[1;32m    147\u001b[0m method \u001b[38;5;241m=\u001b[39m perturbation_methods\u001b[38;5;241m.\u001b[39mget(perturbation_type)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text, {}\n",
      "File \u001b[0;32m~/Desktop/RankingSHAP/RankingShap/perturbation_sentenceLevel_transformer.py:609\u001b[0m, in \u001b[0;36mTransformerPerturbator._apply_adversarial_paraphrase\u001b[0;34m(self, text, intensity)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    606\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    607\u001b[0m                      max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 609\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mintensity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m    621\u001b[0m     candidate \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/transformers/generation/utils.py:2644\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2637\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2638\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2639\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2640\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2641\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2642\u001b[0m     )\n\u001b[1;32m   2643\u001b[0m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2644\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2654\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   2655\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2656\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2657\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/transformers/generation/utils.py:4079\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   4076\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   4077\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m-> 4079\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   4081\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   4082\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   4083\u001b[0m     model_outputs,\n\u001b[1;32m   4084\u001b[0m     model_kwargs,\n\u001b[1;32m   4085\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   4086\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1792\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1792\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1105\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m   1103\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m-> 1105\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:728\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    725\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:342\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    341\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 342\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDenseReluDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:296\u001b[0m, in \u001b[0;36mT5DenseActDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8\n\u001b[1;32m    294\u001b[0m ):\n\u001b[1;32m    295\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 296\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "def example_usage():\n",
    "    \"\"\"Example of transformer-based testing\"\"\"\n",
    "    \n",
    "    # Mock IR model\n",
    "    def mock_model(query: str) -> List[Tuple[str, float]]:\n",
    "        # Simulate retrieval\n",
    "        base_results = [(f\"doc_{i}\", 1.0 - i * 0.1) for i in range(10)]\n",
    "        \n",
    "        # Simulate sensitivity to certain changes\n",
    "        if \"?\" in query and query.count(\"?\") > 1:\n",
    "            random.shuffle(base_results)\n",
    "        \n",
    "        return base_results\n",
    "    \n",
    "    # Initialize tester\n",
    "    print(\"Initializing transformer-based tester...\")\n",
    "    tester = TransformerIRModelTester(mock_model)\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"machine learning algorithms\",\n",
    "        \"How do neural networks work?\",\n",
    "        \"information retrieval evaluation metrics\",\n",
    "        \"transformer models for NLP tasks\"\n",
    "    ]\n",
    "    \n",
    "    # Generate test cases\n",
    "    print(\"\\nGenerating transformer-based perturbations...\")\n",
    "    test_cases = tester.generate_test_cases(\n",
    "        queries=test_queries,\n",
    "        perturbation_types=[\n",
    "            TransformerPerturbationType.T5_PARAPHRASE,\n",
    "            TransformerPerturbationType.ADVERSARIAL_PARAPHRASE,\n",
    "            TransformerPerturbationType.STYLE_TRANSFER,\n",
    "            TransformerPerturbationType.QUERY_EXPANSION\n",
    "        ],\n",
    "        intensity_levels=[0.3, 0.6, 0.9],\n",
    "        samples_per_query=2  # Multiple samples for better statistics\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {len(test_cases)} test cases\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"\\nExample perturbations:\")\n",
    "    for i, tc in enumerate(test_cases[:5]):\n",
    "        print(f\"\\nTest Case {i+1}:\")\n",
    "        print(f\"  Type: {tc.perturbation_type.value}\")\n",
    "        print(f\"  Model: {tc.model_used}\")\n",
    "        print(f\"  Intensity: {tc.intensity}\")\n",
    "        print(f\"  Original: {tc.original_query}\")\n",
    "        print(f\"  Perturbed: {tc.perturbed_query}\")\n",
    "        print(f\"  Semantic Similarity: {tc.semantic_similarity:.3f}\")\n",
    "    \n",
    "    # # Run tests\n",
    "    # print(\"\\nRunning sensitivity tests...\")\n",
    "    # results = tester.run_sensitivity_test(test_cases)\n",
    "    \n",
    "    # # Generate report\n",
    "    # report = tester.generate_report(results)\n",
    "    # print(\"\\n\" + report)\n",
    "    \n",
    "    # # Save results\n",
    "    # with open('transformer_sensitivity_results.json', 'w') as f:\n",
    "    #     # Convert to serializable format\n",
    "    #     serializable_results = {\n",
    "    #         'summary': results['summary'],\n",
    "    #         'semantic_analysis': results['semantic_analysis'],\n",
    "    #         'generation_quality': results['generation_quality'],\n",
    "    #         'sample_perturbations': [\n",
    "    #             {\n",
    "    #                 'original': tc.original_query,\n",
    "    #                 'perturbed': tc.perturbed_query,\n",
    "    #                 'type': tc.perturbation_type.value,\n",
    "    #                 'model': tc.model_used,\n",
    "    #                 'intensity': tc.intensity,\n",
    "    #                 'semantic_similarity': tc.semantic_similarity\n",
    "    #             }\n",
    "    #             for tc in test_cases[:10]\n",
    "    #         ]\n",
    "    #     }\n",
    "    #     json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    # print(\"\\nResults saved to 'transformer_sensitivity_results.json'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b22ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5bfc6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "def example_usage():\n",
    "    \"\"\"Example of sentence-level testing\"\"\"\n",
    "    \n",
    "    # Mock model\n",
    "    def mock_model(query: str) -> List[Tuple[str, float]]:\n",
    "        # Simulate sensitivity to sentence structure\n",
    "        base_results = [(f\"doc_{i}\", 1.0 - i * 0.1) for i in range(10)]\n",
    "        \n",
    "        # Penalize questions vs statements differently\n",
    "        if query.strip().endswith(\"?\"):\n",
    "            base_results = base_results[1:] + [(\"doc_question\", 0.1)]\n",
    "        \n",
    "        # Penalize very long or very short queries\n",
    "        word_count = len(query.split())\n",
    "        if word_count < 3 or word_count > 15:\n",
    "            random.shuffle(base_results)\n",
    "        \n",
    "        return base_results\n",
    "    \n",
    "    # Initialize tester\n",
    "    tester = SentenceLevelIRModelTester(mock_model)\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"The quick brown fox jumps over lazy dogs\",\n",
    "        \"machine learning algorithms\",\n",
    "        \"How do neural networks work?\",\n",
    "        # \"information retrieval systems and their applications\",\n",
    "        # \"latest developments in natural language processing\",\n",
    "        # \"Compare supervised and unsupervised learning methods\"\n",
    "    ]\n",
    "    \n",
    "    # Generate test cases\n",
    "    print(\"Generating sentence-level test cases...\")\n",
    "    test_cases = tester.generate_test_cases(\n",
    "        queries=test_queries,\n",
    "        perturbation_types=[\n",
    "            SentencePerturbationType.PARAPHRASING,\n",
    "            SentencePerturbationType.SENTENCE_EXPANSION,\n",
    "            SentencePerturbationType.SENTENCE_COMPRESSION,\n",
    "            SentencePerturbationType.QUESTION_REFORMULATION,\n",
    "            SentencePerturbationType.STATEMENT_TO_QUESTION,\n",
    "            SentencePerturbationType.QUESTION_TO_STATEMENT,\n",
    "            SentencePerturbationType.ACTIVE_PASSIVE_VOICE,\n",
    "            SentencePerturbationType.TENSE_CHANGE,\n",
    "            SentencePerturbationType.NEGATION,\n",
    "            SentencePerturbationType.EMPHASIS_ADDITION,\n",
    "            SentencePerturbationType.FORMALITY_CHANGE,\n",
    "            SentencePerturbationType.CLAUSE_REORDERING,\n",
    "            SentencePerturbationType.SENTENCE_SPLITTING,\n",
    "            SentencePerturbationType.SENTENCE_MERGING,\n",
    "            SentencePerturbationType.CONTEXT_ADDITION,\n",
    "            SentencePerturbationType.SPECIFICITY_CHANGE,\n",
    "            SentencePerturbationType.SYNONYM_SUBSTITUTION_FULL,\n",
    "            SentencePerturbationType.LINGUISTIC_STYLE_CHANGE\n",
    "        ],\n",
    "        intensity_levels=[0.3, 0.5, 0.7]\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {len(test_cases)} test cases\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"\\nExample transformations:\")\n",
    "    for i, tc in enumerate(test_cases[:50]):\n",
    "        if tc.original_query != tc.perturbed_query:\n",
    "            print(f\"\\nTest Case {i+1}:\")\n",
    "            print(f\"  Type: {tc.perturbation_type.value}\")\n",
    "            print(f\"  Intensity: {tc.intensity}\")\n",
    "            print(f\"  Original: {tc.original_query}\")\n",
    "            print(f\"  Perturbed: {tc.perturbed_query}\")\n",
    "            print(f\"  Semantic Similarity: {tc.semantic_similarity_estimate:.2f}\")\n",
    "    \n",
    "\n",
    "    for test_case in test_cases:\n",
    "        print(f\"Test Case: {test_case.original_query} -> {test_case.perturbed_query} ({test_case.perturbation_type.value}, {test_case.intensity})\")\n",
    "\n",
    "\n",
    "    # # Run tests\n",
    "    # print(f\"\\nRunning tests...\")\n",
    "    # results = tester.run_sensitivity_test(test_cases)\n",
    "    \n",
    "    # # Generate report\n",
    "    # report = tester.generate_report(results)\n",
    "    # print(\"\\n\" + report)\n",
    "    \n",
    "    # # Save results\n",
    "    # with open('sentence_sensitivity_test_results.json', 'w') as f:\n",
    "    #     serializable_results = {\n",
    "    #         'summary': results['summary'],\n",
    "    #         'linguistic_analysis': results['linguistic_analysis'],\n",
    "    #         'semantic_coherence': results['semantic_coherence'],\n",
    "    #         'sample_transformations': [\n",
    "    #             {\n",
    "    #                 'original': tc.original_query,\n",
    "    #                 'perturbed': tc.perturbed_query,\n",
    "    #                 'type': tc.perturbation_type.value,\n",
    "    #                 'intensity': tc.intensity,\n",
    "    #                 'details': tc.transformation_details\n",
    "    #             }\n",
    "    #             for tc in test_cases[:10]\n",
    "    #             if tc.original_query != tc.perturbed_query\n",
    "    #         ]\n",
    "    #     }\n",
    "    #     json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    # print(\"\\nResults saved to 'sentence_sensitivity_test_results.json'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f46e1bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing transformer-based tester...\n",
      "Using device: cpu\n",
      "Loading sentence transformer...\n",
      "\n",
      "Generating transformer-based perturbations...\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading ramsrigouthamg/t5_paraphraser...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Loading t5-base...\n",
      "Error generating perturbation: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Generated 0 test cases\n",
      "\n",
      "Example perturbations:\n",
      "\n",
      "Running sensitivity tests...\n",
      "\n",
      "=== Transformer-Based IR Model Sensitivity Test Report ===\n",
      "\n",
      "Overall Performance:\n",
      "  Average Degradation: 0.000\n",
      "  Std Dev: 0.000\n",
      "  Average NDCG: 0.000\n",
      "  Average MRR: 0.000\n",
      "  Average Semantic Similarity: 0.000\n",
      "\n",
      "Performance by Perturbation Type:\n",
      "\n",
      "Performance by Model:\n",
      "\n",
      "Semantic Preservation Analysis:\n",
      "\n",
      "Generation Quality Analysis:\n",
      "  Length Consistency: nan\n",
      "  Extreme Length Changes: 0\n",
      "\n",
      "Recommendations:\n",
      "\n",
      "Results saved to 'transformer_sensitivity_results.json'\n"
     ]
    }
   ],
   "source": [
    "example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "931e23ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: transformer.layer.*.rel_attn.layer_norm.bias, transformer.layer.*.rel_attn.o, transformer.mask_emb, transformer.layer.*.rel_attn.q, transformer.layer.*.rel_attn.k, transformer.layer.*.rel_attn.seg_embed, lm_loss.weight, transformer.layer.*.ff.layer_*.weight, transformer.layer.*.ff.layer_*.bias, transformer.layer.*.rel_attn.v, transformer.word_embedding.weight, transformer.layer.*.ff.layer_norm.weight, transformer.layer.*.rel_attn.r, transformer.layer.*.rel_attn.layer_norm.weight, transformer.layer.*.ff.layer_norm.bias, transformer.layer.*.rel_attn.r_s_bias, transformer.layer.*.rel_attn.r_r_bias, transformer.layer.*.rel_attn.r_w_bias, lm_loss.bias\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'tokens': Can't extract `str` to `Vec`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# model_path: xlnet-base-cased or gpt2\u001b[39;00m\n\u001b[1;32m     17\u001b[0m aug \u001b[38;5;241m=\u001b[39m nas\u001b[38;5;241m.\u001b[39mContextualWordEmbsForSentenceAug(model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlnet-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m)    \u001b[38;5;66;03m# <-- key change)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m augmented_texts \u001b[38;5;241m=\u001b[39m \u001b[43maug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/nlpaug/base_augmenter.py:98\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[0;34m(self, data, n, num_thread)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbstSummAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBackTranslationAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContextualWordEmbsAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContextualWordEmbsForSentenceAug\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(aug_num):\n\u001b[0;32m---> 98\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43maction_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    100\u001b[0m             augmented_results\u001b[38;5;241m.\u001b[39mextend(result)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/nlpaug/augmenter/sentence/context_word_embs_sentence.py:116\u001b[0m, in \u001b[0;36mContextualWordEmbsForSentenceAug.insert\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    113\u001b[0m     all_data \u001b[38;5;241m=\u001b[39m [data]\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_custom_api:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_custom_insert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_native_insert(all_data)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/nlpaug/augmenter/sentence/context_word_embs_sentence.py:191\u001b[0m, in \u001b[0;36mContextualWordEmbsForSentenceAug._custom_insert\u001b[0;34m(self, all_data)\u001b[0m\n\u001b[1;32m    189\u001b[0m     results \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;241m+\u001b[39m a \u001b[38;5;28;01mfor\u001b[39;00m d, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_data, augmented_texts)]\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlnet\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 191\u001b[0m     results \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_string(a) \u001b[38;5;28;01mfor\u001b[39;00m d, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_data, augmented_texts)]\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# results = [\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m#     d + ' ' + self.model.tokenizer.convert_tokens_to_string(a if isinstance(a, list) else a.split())\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m#     for d, a in zip(all_data, augmented_texts)\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/nlpaug/augmenter/sentence/context_word_embs_sentence.py:191\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    189\u001b[0m     results \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;241m+\u001b[39m a \u001b[38;5;28;01mfor\u001b[39;00m d, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_data, augmented_texts)]\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlnet\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 191\u001b[0m     results \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_data, augmented_texts)]\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# results = [\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m#     d + ' ' + self.model.tokenizer.convert_tokens_to_string(a if isinstance(a, list) else a.split())\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m#     for d, a in zip(all_data, augmented_texts)\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rankingSHAP/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:666\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.convert_tokens_to_string\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert_tokens_to_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 666\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend_tokenizer\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n\u001b[1;32m    669\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'tokens': Can't extract `str` to `Vec`"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"MODEL_DIR\"] = '../model'\n",
    "# import nlpaug\n",
    "# import importlib\n",
    "# importlib.reload(nlpaug)\n",
    "\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "\n",
    "from nlpaug.util import Action\n",
    "\n",
    "\n",
    "text = [\"The\", \"quick brown fox jumps over lazy dogs\"]\n",
    "# model_path: xlnet-base-cased or gpt2\n",
    "aug = nas.ContextualWordEmbsForSentenceAug(model_path='xlnet-base-cased')    # <-- key change)\n",
    "augmented_texts = aug.augment(text, n=2)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Texts:\")\n",
    "print(augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064bdf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c4808021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def example_ir_pipeline(query: str) -> Tuple[List[str], List[float]]:\n",
    "#     \"\"\"Example IR pipeline - replace with your actual implementation\"\"\"\n",
    "#     # Simulate your hybrid IR pipeline\n",
    "#     candidates = [\n",
    "#         f\"Document about {query} - result 1\",\n",
    "#         f\"Information on {query} - result 2\", \n",
    "#         f\"Guide to {query} - result 3\",\n",
    "#         f\"FAQ about {query} - result 4\",\n",
    "#         f\"Details on {query} - result 5\"\n",
    "#     ]\n",
    "#     scores = [0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "#     return candidates, scores\n",
    "\n",
    "# # Initialize tester\n",
    "# tester = HybridIRTester(example_ir_pipeline)\n",
    "\n",
    "# # Run tests\n",
    "# test_queries = [\n",
    "#     \"How can I apply for a mortgage?\",\n",
    "#     \"What are the branch opening hours?\", \n",
    "#     \"Lost my debit card, what should I do?\",\n",
    "#     \"Interest rates for student loans\",\n",
    "#     \"Online banking login issues\"\n",
    "# ]\n",
    "\n",
    "# # Run perturbation tests\n",
    "# print(\"Running perturbation tests...\")\n",
    "# results = tester.run_perturbation_tests(test_queries)\n",
    "# print(results)\n",
    "\n",
    "# # Generate report\n",
    "# # print(\"Generating robustness report...\")\n",
    "# # report = tester.generate_robustness_report()\n",
    "# # print(json.dumps(report, indent=2))\n",
    "\n",
    "# # # Test OOD performance\n",
    "# # ood_queries = [\n",
    "# #     \"Weather forecast for tomorrow\",\n",
    "# #     \"Best restaurants in the city\",\n",
    "# #     \"Stock market trends\"\n",
    "# # ]\n",
    "# # ood_results = tester.test_ood_domain_performance(ood_queries)\n",
    "# # print(\"OOD Performance:\", ood_results)\n",
    "\n",
    "# # # Analyze failure modes\n",
    "# # failure_analysis = tester.analyze_failure_modes()\n",
    "# # print(\"Failure Analysis:\", failure_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66ac5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "def example_usage():\n",
    "    \"\"\"Example of how to use the testing framework\"\"\"\n",
    "    \n",
    "    # Mock model interface (replace with your actual model)\n",
    "    def mock_model(query: str) -> List[Tuple[str, float]]:\n",
    "        # Simulate retrieval results\n",
    "        # In real usage, this would call your BM25 + dense search + cross-encoder pipeline\n",
    "        results = [\n",
    "            (f\"doc_{i}\", 1.0 - i * 0.1) \n",
    "            for i in range(10)\n",
    "        ]\n",
    "        \n",
    "        # Simulate sensitivity to perturbations\n",
    "        if query != query.lower():  # Case sensitive\n",
    "            results = results[1:] + [(\"doc_extra\", 0.1)]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # Initialize tester\n",
    "    tester = HybridIRModelTester(mock_model)\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"machine learning algorithms\",\n",
    "        \"natural language processing\",\n",
    "        \"information retrieval systems\",\n",
    "        \"deep learning neural networks\"\n",
    "    ]\n",
    "    \n",
    "    # Generate test cases\n",
    "    print(\"Generating test cases...\")\n",
    "    test_cases = tester.generate_test_cases(\n",
    "        queries=test_queries,\n",
    "        perturbation_types={\n",
    "            PerturbationType.TYPO: [0.05, 0.1, 0.2],\n",
    "            PerturbationType.DELETION: [0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "            PerturbationType.CASE_CHANGE: [0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "            # PerturbationType.UNICODE_SUBSTITUTION: [0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "            PerturbationType.INSERTION: [0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "            # PerturbationType.SUBSTITUTION: [0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "            PerturbationType.TRANSPOSITION: [0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "            PerturbationType.DUPLICATION: [0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "            PerturbationType.CASE_CHANGE: [0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "            PerturbationType.WHITESPACE_NOISE: [0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "            PerturbationType.PUNCTUATION_NOISE: [0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "        },\n",
    "        # intensity_levels=[0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "    )\n",
    "    # print(test_cases)\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        print(f\"Test Case: {test_case.original_query} -> {test_case.perturbed_query} ({test_case.perturbation_type.value}, {test_case.intensity})\")\n",
    "    # # Run tests\n",
    "    # print(f\"Running {len(test_cases)} test cases...\")\n",
    "    # results = tester.run_sensitivity_test(test_cases)\n",
    "    \n",
    "    # # Generate report\n",
    "    # report = tester.generate_report(results)\n",
    "    # print(\"\\n\" + report)\n",
    "    \n",
    "    # # Save detailed results\n",
    "    # with open('sensitivity_test_results.json', 'w') as f:\n",
    "    #     # Convert test cases to serializable format\n",
    "    #     serializable_results = {\n",
    "    #         'summary': results['summary'],\n",
    "    #         'details': [\n",
    "    #             {\n",
    "    #                 'original_query': r['test_case'].original_query,\n",
    "    #                 'perturbed_query': r['test_case'].perturbed_query,\n",
    "    #                 'perturbation_type': r['test_case'].perturbation_type.value,\n",
    "    #                 'intensity': r['test_case'].intensity,\n",
    "    #                 'metrics': r['metrics']\n",
    "    #             }\n",
    "    #             for r in results['details']\n",
    "    #         ]\n",
    "    #     }\n",
    "    #     json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    # print(\"\\nDetailed results saved to 'sensitivity_test_results.json'\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f6ddf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test cases...\n",
      "Test Case: machine learning algorithms -> machine learning algirithms (typo, 0.05)\n",
      "Test Case: machine learning algorithms -> jachine learning algorithms (typo, 0.1)\n",
      "Test Case: machine learning algorithms -> mady8ne learning algori5hms (typo, 0.2)\n",
      "Test Case: machine learning algorithms -> machine learnig algorithms (deletion, 0.05)\n",
      "Test Case: machine learning algorithms -> chine learning algorithms (deletion, 0.1)\n",
      "Test Case: machine learning algorithms -> mahine erning algoiths (deletion, 0.2)\n",
      "Test Case: machine learning algorithms -> achine erning lgths (deletion, 0.3)\n",
      "Test Case: machine learning algorithms -> ai laig githms (deletion, 0.5)\n",
      "Test Case: machine learning algorithms -> macHine learning algorithms (case_change, 0.05)\n",
      "Test Case: machine learning algorithms -> machine learning algorithms (case_change, 0.1)\n",
      "Test Case: machine learning algorithms -> mAchine Learning alGoriThMs (case_change, 0.2)\n",
      "Test Case: machine learning algorithms -> maCHine lEarNiNg algorithmS (case_change, 0.3)\n",
      "Test Case: machine learning algorithms -> mAchinE lEaRning aLGORItHms (case_change, 0.5)\n",
      "Test Case: machine learning algorithms -> machine learning algorithmss (insertion, 0.05)\n",
      "Test Case: machine learning algorithms -> macchine learnming algorithms (insertion, 0.1)\n",
      "Test Case: machine learning algorithms -> machii8ne rlearning ualgorithmsa (insertion, 0.2)\n",
      "Test Case: machine learning algorithms -> machinew lear4nmiingg algorriithmsz (insertion, 0.3)\n",
      "Test Case: machine learning algorithms -> macxxhine3qa elewarnionng allggoritthmsz (insertion, 0.5)\n",
      "Test Case: machine learning algorithms -> machine learning algoirthms (transposition, 0.05)\n",
      "Test Case: machine learning algorithms -> macihne learning algorihtms (transposition, 0.1)\n",
      "Test Case: machine learning algorithms -> machine elarning lagoirthms (transposition, 0.2)\n",
      "Test Case: machine learning algorithms -> amcihen lreanngi algroithms (transposition, 0.3)\n",
      "Test Case: machine learning algorithms -> macihen ealrninga logritmhs (transposition, 0.5)\n",
      "Test Case: machine learning algorithms -> machinee learning algorithms (duplication, 0.05)\n",
      "Test Case: machine learning algorithms -> machiine leaarning algorithms (duplication, 0.1)\n",
      "Test Case: machine learning algorithms -> machinne learning  algoriithhmms (duplication, 0.2)\n",
      "Test Case: machine learning algorithms -> mmmachinne leearniing  aaalgorithms (duplication, 0.3)\n",
      "Test Case: machine learning algorithms -> maachhiiiiine  leearnning algorithmmmmss (duplication, 0.5)\n",
      "Test Case: machine learning algorithms -> machinelearning algorithms (whitespace_noise, 0.05)\n",
      "Test Case: machine learning algorithms -> machine learning algorithms  (whitespace_noise, 0.1)\n",
      "Test Case: machine learning algorithms -> machine  learning algorithms (whitespace_noise, 0.2)\n",
      "Test Case: machine learning algorithms -> machinelearning algorithms (whitespace_noise, 0.3)\n",
      "Test Case: machine learning algorithms -> machine learningalgorithms (whitespace_noise, 0.5)\n",
      "Test Case: machine learning algorithms -> machine learning algorithms (punctuation_noise, 0.05)\n",
      "Test Case: machine learning algorithms -> machine learning algorithms (punctuation_noise, 0.1)\n",
      "Test Case: machine learning algorithms -> machine/ l\"earning algorithms (punctuation_noise, 0.2)\n",
      "Test Case: machine learning algorithms -> ma<chine learning a.lgorithms (punctuation_noise, 0.3)\n",
      "Test Case: machine learning algorithms -> machine- lear@nin{g algo'ri,thms (punctuation_noise, 0.5)\n",
      "Test Case: natural language processing -> natural panguage processing (typo, 0.05)\n",
      "Test Case: natural language processing -> nayural language processing (typo, 0.1)\n",
      "Test Case: natural language processing -> naryral language prodessihf (typo, 0.2)\n",
      "Test Case: natural language processing -> naturl language processing (deletion, 0.05)\n",
      "Test Case: natural language processing -> natural languae pocessing (deletion, 0.1)\n",
      "Test Case: natural language processing -> ntura anguae rocessing (deletion, 0.2)\n",
      "Test Case: natural language processing -> atural lagg prossin (deletion, 0.3)\n",
      "Test Case: natural language processing -> al ngage rcsig (deletion, 0.5)\n",
      "Test Case: natural language processing -> natural languagE processing (case_change, 0.05)\n",
      "Test Case: natural language processing -> natural language processing (case_change, 0.1)\n",
      "Test Case: natural language processing -> nATural language processing (case_change, 0.2)\n",
      "Test Case: natural language processing -> natUral langUage pROcEssing (case_change, 0.3)\n",
      "Test Case: natural language processing -> nATural LaNguAge pRocEssing (case_change, 0.5)\n",
      "Test Case: natural language processing -> naturaql language processing (insertion, 0.05)\n",
      "Test Case: natural language processing -> natuirall language processing (insertion, 0.1)\n",
      "Test Case: natural language processing -> nattural langfuazage procesasing (insertion, 0.2)\n",
      "Test Case: natural language processing -> vnatureall l1annbgtuasge processing (insertion, 0.3)\n",
      "Test Case: natural language processing -> knat5turaqql1 jllanguage p0preocewssingg (insertion, 0.5)\n",
      "Test Case: natural language processing -> natural language porcessing (transposition, 0.05)\n",
      "Test Case: natural language processing -> natuarl language rpocessing (transposition, 0.1)\n",
      "Test Case: natural language processing -> ntauarl lnauggae processing (transposition, 0.2)\n",
      "Test Case: natural language processing -> ntarua llanugage processign (transposition, 0.3)\n",
      "Test Case: natural language processing -> natuarll anugage rpcoesnsig (transposition, 0.5)\n",
      "Test Case: natural language processing -> nnatural language processing (duplication, 0.05)\n",
      "Test Case: natural language processing -> naturaal language processsing (duplication, 0.1)\n",
      "Test Case: natural language processing -> natural lannguuage proocesssiing (duplication, 0.2)\n",
      "Test Case: natural language processing -> natturaaal   language proccesssiing (duplication, 0.3)\n",
      "Test Case: natural language processing -> naaturall  laanguuuagee  pproocesssinnng (duplication, 0.5)\n",
      "Test Case: natural language processing -> natural  language processing (whitespace_noise, 0.05)\n",
      "Test Case: natural language processing -> natural languageprocessing (whitespace_noise, 0.1)\n",
      "Test Case: natural language processing -> natural language processing  (whitespace_noise, 0.2)\n",
      "Test Case: natural language processing -> natural language processing  (whitespace_noise, 0.3)\n",
      "Test Case: natural language processing -> natural  language processing (whitespace_noise, 0.5)\n",
      "Test Case: natural language processing -> natural ,language processing (punctuation_noise, 0.05)\n",
      "Test Case: natural language processing -> natural language processing (punctuation_noise, 0.1)\n",
      "Test Case: natural language processing -> natural language processing (punctuation_noise, 0.2)\n",
      "Test Case: natural language processing -> natural language proc(essing (punctuation_noise, 0.3)\n",
      "Test Case: natural language processing -> natura,l language processing (punctuation_noise, 0.5)\n",
      "Test Case: information retrieval systems -> information retrievzl systems (typo, 0.05)\n",
      "Test Case: information retrieval systems -> informatipn retriwval systems (typo, 0.1)\n",
      "Test Case: information retrieval systems -> informqtipm retrievzl sys5ems (typo, 0.2)\n",
      "Test Case: information retrieval systems -> information retieval systems (deletion, 0.05)\n",
      "Test Case: information retrieval systems -> information retieval sytems (deletion, 0.1)\n",
      "Test Case: information retrieval systems -> infortion retrial system (deletion, 0.2)\n",
      "Test Case: information retrieval systems -> inormatio rtrievl sss (deletion, 0.3)\n",
      "Test Case: information retrieval systems -> nran etrevl sys (deletion, 0.5)\n",
      "Test Case: information retrieval systems -> information retrieval syStems (case_change, 0.05)\n",
      "Test Case: information retrieval systems -> inFormation reTrieval systems (case_change, 0.1)\n",
      "Test Case: information retrieval systems -> informAtion rEtrieVaL systemS (case_change, 0.2)\n",
      "Test Case: information retrieval systems -> infoRmaTion retRievaL systems (case_change, 0.3)\n",
      "Test Case: information retrieval systems -> InfOrMaTion rEtrieval sySTeMs (case_change, 0.5)\n",
      "Test Case: information retrieval systems -> information retrrieval systems (insertion, 0.05)\n",
      "Test Case: information retrieval systems -> information retriieval systtems (insertion, 0.1)\n",
      "Test Case: information retrieval systems -> info0rmation ret5rierval sysztemss (insertion, 0.2)\n",
      "Test Case: information retrieval systems -> informat5ion retrr4hieval systteemmjs (insertion, 0.3)\n",
      "Test Case: information retrieval systems -> i8nfoormation incretrieevbaal1l ssy6sstyems (insertion, 0.5)\n",
      "Test Case: information retrieval systems -> information rterieval systems (transposition, 0.05)\n",
      "Test Case: information retrieval systems -> niformation retireval systems (transposition, 0.1)\n",
      "Test Case: information retrieval systems -> informatio nertrieval sysetms (transposition, 0.2)\n",
      "Test Case: information retrieval systems -> infoarmtion errtivela ssytmes (transposition, 0.3)\n",
      "Test Case: information retrieval systems -> ionfmartion etrrievasl ystesm (transposition, 0.5)\n",
      "Test Case: information retrieval systems -> informatiion retrieval systems (duplication, 0.05)\n",
      "Test Case: information retrieval systems -> informationn rretrieval systems (duplication, 0.1)\n",
      "Test Case: information retrieval systems -> informaatioon reetrievvall systems (duplication, 0.2)\n",
      "Test Case: information retrieval systems -> innformmatiion retrrieval syyssttemss (duplication, 0.3)\n",
      "Test Case: information retrieval systems -> innnnnfoorrmatiiion  reetrieevall syystemss (duplication, 0.5)\n",
      "Test Case: information retrieval systems -> information retrieval systems  (whitespace_noise, 0.05)\n",
      "Test Case: information retrieval systems -> information  retrieval systems (whitespace_noise, 0.1)\n",
      "Test Case: information retrieval systems -> information retrieval  systems (whitespace_noise, 0.2)\n",
      "Test Case: information retrieval systems -> informationretrieval systems (whitespace_noise, 0.3)\n",
      "Test Case: information retrieval systems -> informationretrieval systems (whitespace_noise, 0.5)\n",
      "Test Case: information retrieval systems -> informati'on retrieval systems (punctuation_noise, 0.05)\n",
      "Test Case: information retrieval systems -> information retrieval systems> (punctuation_noise, 0.1)\n",
      "Test Case: information retrieval systems -> information retrieval systems (punctuation_noise, 0.2)\n",
      "Test Case: information retrieval systems -> information r\\etrieval systems (punctuation_noise, 0.3)\n",
      "Test Case: information retrieval systems -> information retrieval sy@stems (punctuation_noise, 0.5)\n",
      "Test Case: deep learning neural networks -> feep learning neural networks (typo, 0.05)\n",
      "Test Case: deep learning neural networks -> deep pearning neural networls (typo, 0.1)\n",
      "Test Case: deep learning neural networks -> deep lrarnong neural nrtwprkz (typo, 0.2)\n",
      "Test Case: deep learning neural networks -> deep learning neura networks (deletion, 0.05)\n",
      "Test Case: deep learning neural networks -> eep learning neura networks (deletion, 0.1)\n",
      "Test Case: deep learning neural networks -> dep lernng neurl ntworks (deletion, 0.2)\n",
      "Test Case: deep learning neural networks -> deep leaning neu etwo (deletion, 0.3)\n",
      "Test Case: deep learning neural networks -> ee lear nel tok (deletion, 0.5)\n",
      "Test Case: deep learning neural networks -> deep learnIng neural networks (case_change, 0.05)\n",
      "Test Case: deep learning neural networks -> deep learniNg neural nEtworks (case_change, 0.1)\n",
      "Test Case: deep learning neural networks -> deep leArNing Neural networks (case_change, 0.2)\n",
      "Test Case: deep learning neural networks -> deep lEarnIng neuRal netWorks (case_change, 0.3)\n",
      "Test Case: deep learning neural networks -> deeP LeaRNing nEUrAL nEtWorks (case_change, 0.5)\n",
      "Test Case: deep learning neural networks -> deep learning neural netwoorks (insertion, 0.05)\n",
      "Test Case: deep learning neural networks -> deep leeaarning neural networks (insertion, 0.1)\n",
      "Test Case: deep learning neural networks -> sdceep learning neuyural networkks (insertion, 0.2)\n",
      "Test Case: deep learning neural networks -> deeee3ep learrningg neural networrksa (insertion, 0.3)\n",
      "Test Case: deep learning neural networks -> ndee3pp yleewarnmi8ingh neuralp neettweorks (insertion, 0.5)\n",
      "Test Case: deep learning neural networks -> deep learning neural networsk (transposition, 0.05)\n",
      "Test Case: deep learning neural networks -> depe learning enural networks (transposition, 0.1)\n",
      "Test Case: deep learning neural networks -> deep lernaing neural entworks (transposition, 0.2)\n",
      "Test Case: deep learning neural networks -> dee plaerningn eulrane twroks (transposition, 0.3)\n",
      "Test Case: deep learning neural networks -> dpee elrniang neuranl ewtkros (transposition, 0.5)\n",
      "Test Case: deep learning neural networks -> deep learning neuraal networks (duplication, 0.05)\n",
      "Test Case: deep learning neural networks -> deep learnning  neural networks (duplication, 0.1)\n",
      "Test Case: deep learning neural networks -> deepp learning  neurral  nnetworks (duplication, 0.2)\n",
      "Test Case: deep learning neural networks -> deeep llearrning neeeural neetwoorkss (duplication, 0.3)\n",
      "Test Case: deep learning neural networks -> deeeeep  leaarniinngg neeuraal  neettworkks (duplication, 0.5)\n",
      "Test Case: deep learning neural networks -> deep learningneural networks (whitespace_noise, 0.05)\n",
      "Test Case: deep learning neural networks -> deep learning neural networks  (whitespace_noise, 0.1)\n",
      "Test Case: deep learning neural networks -> deep learningneural networks (whitespace_noise, 0.2)\n",
      "Test Case: deep learning neural networks -> deep learningneural networks (whitespace_noise, 0.3)\n",
      "Test Case: deep learning neural networks -> deep learning neural   networks (whitespace_noise, 0.5)\n",
      "Test Case: deep learning neural networks -> deep learning neural networks (punctuation_noise, 0.05)\n",
      "Test Case: deep learning neural networks -> deep learning neural networks (punctuation_noise, 0.1)\n",
      "Test Case: deep learning neural networks -> deep learning neural networks (punctuation_noise, 0.2)\n",
      "Test Case: deep learning neural networks -> deep learning ne\\ural networks (punctuation_noise, 0.3)\n",
      "Test Case: deep learning neural networks -> deep l<earning neural networks (punctuation_noise, 0.5)\n"
     ]
    }
   ],
   "source": [
    "example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "705f2f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06402589632634492"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "bf29f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/keqiaoli/Desktop/RankingSHAP/RankingShap/')\n",
    "import importlib\n",
    "importlib.reload(try6)\n",
    "import try6\n",
    "from try6 import *\n",
    "import perturbation_wordLevel\n",
    "importlib.reload(perturbation_wordLevel)\n",
    "\n",
    "from perturbation_wordLevel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45778b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "def example_usage():\n",
    "    \"\"\"Example of how to use the word-level testing framework\"\"\"\n",
    "    \n",
    "    # Mock model interface\n",
    "    def mock_model(query: str) -> List[Tuple[str, float]]:\n",
    "        # Simulate retrieval results\n",
    "        base_results = [\n",
    "            (f\"doc_{i}\", 1.0 - i * 0.1) \n",
    "            for i in range(10)\n",
    "        ]\n",
    "        # Simulate sensitivity to word changes\n",
    "        words = query.lower().split()\n",
    "        \n",
    "        # Penalize for missing important words\n",
    "        important_words = {'machine', 'learning', 'information', 'retrieval'}\n",
    "        missing_important = len(important_words - set(words))\n",
    "        \n",
    "        if missing_important > 0:\n",
    "            # Shuffle results based on missing words\n",
    "            base_results = base_results[missing_important:] + base_results[:missing_important]\n",
    "        \n",
    "        return base_results\n",
    "    \n",
    "    # Initialize tester\n",
    "    tester = WordLevelIRModelTester(mock_model)\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        # \"machine learning algorithms for text classification\",\n",
    "        # \"information retrieval systems and search engines\",\n",
    "        # \"natural language processing with deep learning\",\n",
    "        # \"large scale data mining techniques\",\n",
    "        # \"neural network architectures for computer vision\",\n",
    "        \"The quick brown fox jumps over lazy dogs\",\n",
    "        # \"machine learning algorithms\",\n",
    "        # \"natural language processing\",\n",
    "        # \"information retrieval systems\",\n",
    "        # \"deep learning neural networks\"\n",
    "    ]\n",
    "    \n",
    "    # Generate test cases with specific perturbations\n",
    "    print(\"Generating word-level test cases...\")\n",
    "    test_cases = tester.generate_test_cases(\n",
    "        queries=test_queries,\n",
    "        perturbation_types=[\n",
    "            WordPerturbationType.SYNONYM_REPLACEMENT,\n",
    "            WordPerturbationType.WORD_DELETION,\n",
    "            WordPerturbationType.WORD_INSERTION,\n",
    "            WordPerturbationType.WORD_REORDERING,\n",
    "            WordPerturbationType.RANDOM_WORD_REPLACEMENT,\n",
    "            WordPerturbationType.WORD_SPLITTING,\n",
    "            WordPerturbationType.WORD_MERGING,\n",
    "            # WordPerturbationType.ABBREVIATION,\n",
    "        ],\n",
    "        intensity_levels=[0.1, 0.2, 0.3, 0.5]\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {len(test_cases)} test cases\")\n",
    "    \n",
    "    # Show some example perturbations\n",
    "    print(\"\\nExample perturbations:\")\n",
    "    for i, tc in enumerate(test_cases[:20]):\n",
    "        print(f\"\\nTest Case {i+1}:\")\n",
    "        print(f\"  Type: {tc.perturbation_type.value}\")\n",
    "        print(f\"  Intensity: {tc.intensity}\")\n",
    "        print(f\"  Original: {tc.original_query}\")\n",
    "        print(f\"  Perturbed: {tc.perturbed_query}\")\n",
    "        print(f\"  Affected: {tc.affected_words}\")\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        print(f\"Test Case: {test_case.original_query} -> {test_case.perturbed_query} ({test_case.perturbation_type.value}, {test_case.intensity})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd76cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word-level test cases...\n",
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dogs']\n",
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dogs']\n",
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dogs']\n",
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dogs']\n",
      "Generated 28 test cases\n",
      "\n",
      "Example perturbations:\n",
      "\n",
      "Test Case 1:\n",
      "  Type: synonym_replacement\n",
      "  Intensity: 0.1\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The quick brown fox jumps over lazy dogs\n",
      "  Affected: []\n",
      "\n",
      "Test Case 2:\n",
      "  Type: synonym_replacement\n",
      "  Intensity: 0.2\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The quick brown fox jumps over lazy dogs\n",
      "  Affected: []\n",
      "\n",
      "Test Case 3:\n",
      "  Type: synonym_replacement\n",
      "  Intensity: 0.3\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The quick brown fox jumps over lazy dogs\n",
      "  Affected: []\n",
      "\n",
      "Test Case 4:\n",
      "  Type: synonym_replacement\n",
      "  Intensity: 0.5\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The quick brownness dodger jumps over lazy dogs\n",
      "  Affected: ['fox', 'brown']\n",
      "\n",
      "Test Case 5:\n",
      "  Type: word_deletion\n",
      "  Intensity: 0.1\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: quick brown fox jumps over lazy dogs\n",
      "  Affected: ['The']\n",
      "\n",
      "Test Case 6:\n",
      "  Type: word_deletion\n",
      "  Intensity: 0.2\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: quick brown fox jumps over lazy dogs\n",
      "  Affected: ['The']\n",
      "\n",
      "Test Case 7:\n",
      "  Type: word_deletion\n",
      "  Intensity: 0.3\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: brown fox jumps over lazy dogs\n",
      "  Affected: ['The', 'quick']\n",
      "\n",
      "Test Case 8:\n",
      "  Type: word_deletion\n",
      "  Intensity: 0.5\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: jumps over lazy dogs\n",
      "  Affected: ['The', 'quick', 'brown', 'fox']\n",
      "\n",
      "Test Case 9:\n",
      "  Type: word_insertion\n",
      "  Intensity: 0.1\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The quick brown fox jumps over significantly lazy dogs\n",
      "  Affected: []\n",
      "\n",
      "Test Case 10:\n",
      "  Type: word_insertion\n",
      "  Intensity: 0.2\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The actually quick brown fox jumps over lazy various dogs\n",
      "  Affected: []\n",
      "\n",
      "Test Case 11:\n",
      "  Type: word_insertion\n",
      "  Intensity: 0.3\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The quick designated brown fox jumps over very lazy dogs\n",
      "  Affected: []\n",
      "\n",
      "Test Case 12:\n",
      "  Type: word_insertion\n",
      "  Intensity: 0.5\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The really quick brown fox generally jumps over particularly lazy necessary dogs\n",
      "  Affected: []\n",
      "\n",
      "Test Case 13:\n",
      "  Type: word_reordering\n",
      "  Intensity: 0.1\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The quick brown fox jumps over lazy dogs\n",
      "  Affected: []\n",
      "\n",
      "Test Case 14:\n",
      "  Type: word_reordering\n",
      "  Intensity: 0.2\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The brown quick fox jumps over lazy dogs\n",
      "  Affected: ['quick↔brown']\n",
      "\n",
      "Test Case 15:\n",
      "  Type: word_reordering\n",
      "  Intensity: 0.3\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The brown quick fox jumps over lazy dogs\n",
      "  Affected: ['quick↔brown']\n",
      "\n",
      "Test Case 16:\n",
      "  Type: word_reordering\n",
      "  Intensity: 0.5\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The quick brown jumps fox lazy over dogs\n",
      "  Affected: ['fox↔jumps', 'over↔lazy']\n",
      "\n",
      "Test Case 17:\n",
      "  Type: random_word_replacement\n",
      "  Intensity: 0.1\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The quick brown fox jumps brownish lazy dogs\n",
      "  Affected: ['over']\n",
      "\n",
      "Test Case 18:\n",
      "  Type: random_word_replacement\n",
      "  Intensity: 0.2\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The jeanpaulia brown fox jumps over lazy dogs\n",
      "  Affected: ['quick']\n",
      "\n",
      "Test Case 19:\n",
      "  Type: random_word_replacement\n",
      "  Intensity: 0.3\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: The unsectarianize hypsilophodontid fox jumps over lazy dogs\n",
      "  Affected: ['quick', 'brown']\n",
      "\n",
      "Test Case 20:\n",
      "  Type: random_word_replacement\n",
      "  Intensity: 0.5\n",
      "  Original: The quick brown fox jumps over lazy dogs\n",
      "  Perturbed: Jadedly quick brown benchboard pearceite over ceratophrys dogs\n",
      "  Affected: ['jumps', 'lazy', 'The', 'fox']\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quick brown fox jumps over lazy dogs (synonym_replacement, 0.1)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quick brown fox jumps over lazy dogs (synonym_replacement, 0.2)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quick brown fox jumps over lazy dogs (synonym_replacement, 0.3)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quick brownness dodger jumps over lazy dogs (synonym_replacement, 0.5)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> quick brown fox jumps over lazy dogs (word_deletion, 0.1)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> quick brown fox jumps over lazy dogs (word_deletion, 0.2)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> brown fox jumps over lazy dogs (word_deletion, 0.3)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> jumps over lazy dogs (word_deletion, 0.5)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quick brown fox jumps over significantly lazy dogs (word_insertion, 0.1)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The actually quick brown fox jumps over lazy various dogs (word_insertion, 0.2)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quick designated brown fox jumps over very lazy dogs (word_insertion, 0.3)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The really quick brown fox generally jumps over particularly lazy necessary dogs (word_insertion, 0.5)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quick brown fox jumps over lazy dogs (word_reordering, 0.1)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The brown quick fox jumps over lazy dogs (word_reordering, 0.2)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The brown quick fox jumps over lazy dogs (word_reordering, 0.3)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quick brown jumps fox lazy over dogs (word_reordering, 0.5)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quick brown fox jumps brownish lazy dogs (random_word_replacement, 0.1)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The jeanpaulia brown fox jumps over lazy dogs (random_word_replacement, 0.2)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The unsectarianize hypsilophodontid fox jumps over lazy dogs (random_word_replacement, 0.3)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> Jadedly quick brown benchboard pearceite over ceratophrys dogs (random_word_replacement, 0.5)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quick brown fox jum ps over lazy dogs (word_splitting, 0.1)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quick brown f ox jumps o ver lazy dogs (word_splitting, 0.2)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quick brown f ox jumps over la zy dogs (word_splitting, 0.3)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> T he quick brown fo x jumps over la zy do gs (word_splitting, 0.5)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quick brown fox jumps over lazy dogs (word_merging, 0.1)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quickbrown fox jumps over lazy dogs (word_merging, 0.2)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> The quick brown foxjumps overlazy dogs (word_merging, 0.3)\n",
      "Test Case: The quick brown fox jumps over lazy dogs -> Thequick brownfox jumpsover lazy dogs (word_merging, 0.5)\n"
     ]
    }
   ],
   "source": [
    "example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "77a2588a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/keqiaoli/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1503c718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('machine.n.01.machine')\n",
      "Lemma('machine.n.02.machine')\n",
      "Lemma('machine.n.03.machine')\n",
      "Lemma('machine.n.04.machine')\n",
      "Lemma('machine.n.04.simple_machine')\n",
      "Lemma('machine.n.05.machine')\n",
      "Lemma('machine.n.05.political_machine')\n",
      "Lemma('car.n.01.car')\n",
      "Lemma('car.n.01.auto')\n",
      "Lemma('car.n.01.automobile')\n",
      "Lemma('car.n.01.machine')\n",
      "Lemma('car.n.01.motorcar')\n",
      "Lemma('machine.v.01.machine')\n",
      "Lemma('machine.v.02.machine')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "word = 'machine'\n",
    "\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(word.lower()):\n",
    "    # print(syn)\n",
    "    for lemma in syn.lemmas():\n",
    "        print(lemma)\n",
    "        synonym = lemma.name().replace('_', ' ')\n",
    "        if synonym.lower() != word.lower():\n",
    "            synonyms.append(synonym)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3231f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/keqiaoli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/keqiaoli/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning live angstrom key out contribution of Bodoni font AI, but deep learning is flush more than popular.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def replace_with_synonyms(sentence, replace_prob=0.3, exclude_phrases=None):\n",
    "    if exclude_phrases is None:\n",
    "        exclude_phrases = []\n",
    "\n",
    "    # Sort longer phrases first so they match before single words\n",
    "    exclude_phrases = sorted(exclude_phrases, key=len, reverse=True)\n",
    "\n",
    "    # Protect excluded phrases by replacing them with placeholders\n",
    "    protected_map = {}\n",
    "    protected_sentence = sentence\n",
    "    for i, phrase in enumerate(exclude_phrases):\n",
    "        placeholder = f\"__PHRASE_{i}__\"\n",
    "        pattern = re.compile(re.escape(phrase), re.IGNORECASE)\n",
    "        protected_sentence = pattern.sub(placeholder, protected_sentence)\n",
    "        protected_map[placeholder] = phrase\n",
    "\n",
    "    # Tokenize (words and punctuation separately)\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", protected_sentence, re.UNICODE)\n",
    "    new_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        # If token is a placeholder, keep it as is\n",
    "        if token in protected_map:\n",
    "            new_tokens.append(token)\n",
    "            continue\n",
    "\n",
    "        # Only replace alphabetic tokens\n",
    "        if token.isalpha() and random.random() < replace_prob:\n",
    "            synonyms = set()\n",
    "            for syn in wordnet.synsets(token.lower()):\n",
    "                for lemma in syn.lemmas():\n",
    "                    lemma_name = lemma.name().replace(\"_\", \" \")\n",
    "                    if lemma_name.lower() != token.lower():\n",
    "                        synonyms.add(lemma_name)\n",
    "\n",
    "            if synonyms:\n",
    "                new_word = random.choice(list(synonyms))\n",
    "                # Preserve capitalization\n",
    "                if token[0].isupper():\n",
    "                    new_word = new_word.capitalize()\n",
    "                new_tokens.append(new_word)\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "\n",
    "    # Join tokens back\n",
    "    replaced_sentence = \"\".join(\n",
    "        [t if re.match(r\"[^\\w\\s]\", t) else \" \" + t for t in new_tokens]\n",
    "    ).strip()\n",
    "\n",
    "    # Restore protected phrases\n",
    "    for placeholder, phrase in protected_map.items():\n",
    "        replaced_sentence = replaced_sentence.replace(placeholder, phrase)\n",
    "\n",
    "    return replaced_sentence\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Machine learning is a key part of modern AI, but deep learning is even more popular.\"\n",
    "exclude_list = [\"machine learning\", \"deep learning\"]\n",
    "\n",
    "print(replace_with_synonyms(sentence, replace_prob=0.5, exclude_phrases=exclude_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f88ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning equal type A central contribution of modern AI, only deep learning is even out more popular.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/keqiaoli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def replace_with_synonyms(sentence, replace_prob=0.3, exclude_phrases=None):\n",
    "    if exclude_phrases is None:\n",
    "        exclude_phrases = []\n",
    "\n",
    "    # Sort longer phrases first so they match before single words\n",
    "    exclude_phrases = sorted(exclude_phrases, key=len, reverse=True)\n",
    "\n",
    "    # Protect excluded phrases by replacing them with placeholders\n",
    "    protected_map = {}\n",
    "    protected_sentence = sentence\n",
    "    for i, phrase in enumerate(exclude_phrases):\n",
    "        placeholder = f\"__PHRASE_{i}__\"\n",
    "        pattern = re.compile(re.escape(phrase), re.IGNORECASE)\n",
    "        protected_sentence = pattern.sub(placeholder, protected_sentence)\n",
    "        protected_map[placeholder] = phrase\n",
    "\n",
    "    # Tokenize (words and punctuation separately)\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", protected_sentence, re.UNICODE)\n",
    "    new_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        # If token is a placeholder, keep it as is\n",
    "        if token in protected_map:\n",
    "            new_tokens.append(token)\n",
    "            continue\n",
    "\n",
    "        # Only replace alphabetic tokens\n",
    "        if token.isalpha() and random.random() < replace_prob:\n",
    "            synonyms = set()\n",
    "            for syn in wordnet.synsets(token.lower()):\n",
    "                for lemma in syn.lemmas():\n",
    "                    lemma_name = lemma.name().replace(\"_\", \" \")\n",
    "                    if lemma_name.lower() != token.lower():\n",
    "                        synonyms.add(lemma_name)\n",
    "\n",
    "            if synonyms:\n",
    "                new_word = random.choice(list(synonyms))\n",
    "                # Preserve capitalization\n",
    "                if token[0].isupper():\n",
    "                    new_word = new_word.capitalize()\n",
    "                new_tokens.append(new_word)\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "\n",
    "    # Join tokens back\n",
    "    replaced_sentence = \"\".join(\n",
    "        [t if re.match(r\"[^\\w\\s]\", t) else \" \" + t for t in new_tokens]\n",
    "    ).strip()\n",
    "\n",
    "    # Restore protected phrases\n",
    "    for placeholder, phrase in protected_map.items():\n",
    "        replaced_sentence = replaced_sentence.replace(placeholder, phrase)\n",
    "\n",
    "    return replaced_sentence\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Machine learning is a key part of modern AI, but deep learning is even more popular.\"\n",
    "exclude_list = [\"machine learning\", \"deep learning\"]\n",
    "\n",
    "print(replace_with_synonyms(sentence, replace_prob=0.5, exclude_phrases=exclude_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3811bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"machine learning's powerful. Deep learning is popular!!!\"\n",
    "\n",
    "tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ea6ece35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine',\n",
       " 'learning',\n",
       " \"'\",\n",
       " 's',\n",
       " 'powerful',\n",
       " '.',\n",
       " 'Deep',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'popular',\n",
       " '!',\n",
       " '!',\n",
       " '!']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1e277ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"machine learning ' s powerful . Deep learning is popular ! ! !\""
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "482c84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- helpers (top of your module) ---\n",
    "_WORD = r\"[A-Za-z]+(?:[-'][A-Za-z]+)*\"          # handles hyphenated words & apostrophes: policy-maker, bank's\n",
    "_PLACEHOLDER = r\"__PHRASE_\\d+__\"\n",
    "_PUNCT = r\"[^\\w\\s]\"                             # any single non-word, non-space (.,;:!?()[]{}\"”’ etc.)\n",
    "\n",
    "TOKEN_RX = re.compile(fr\"{_PLACEHOLDER}|{_WORD}|{_PUNCT}\")\n",
    "\n",
    "def tokenize(text: str):\n",
    "    # Returns a list of tokens: words/placeholders/punctuation\n",
    "    return TOKEN_RX.findall(text)\n",
    "\n",
    "def detokenize(tokens):\n",
    "    out = []\n",
    "    for t in tokens:\n",
    "        if not out:\n",
    "            out.append(t)\n",
    "            continue\n",
    "        # If current token is punctuation, attach to previous without space\n",
    "        if re.fullmatch(_PUNCT, t):\n",
    "            out[-1] += t\n",
    "        else:\n",
    "            # otherwise add a space then the token\n",
    "            out.append(\" \" + t)\n",
    "    return \"\".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "958dd519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine',\n",
       " \"learning's\",\n",
       " 'powerful',\n",
       " '.',\n",
       " 'Deep',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'popular',\n",
       " '!',\n",
       " '!',\n",
       " '!']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2cdb0027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"machine learning's powerful. Deep learning is popular!!!\""
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenize(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc1d68b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RandomWordAug.__init__() got an unexpected keyword argument 'candidate_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 14\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     aug \u001b[38;5;241m=\u001b[39m \u001b[43mnaw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomWordAug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minsertion_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: RandomWordAug.__init__() got an unexpected keyword argument 'custom_vocab'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     aug \u001b[38;5;241m=\u001b[39m naw\u001b[38;5;241m.\u001b[39mRandomWordAug(custom_vocab\u001b[38;5;241m=\u001b[39minsertion_vocab, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     aug \u001b[38;5;241m=\u001b[39m \u001b[43mnaw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomWordAug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minsertion_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe policy must be followed by all employees.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(aug\u001b[38;5;241m.\u001b[39maugment(text))\n",
      "\u001b[0;31mTypeError\u001b[0m: RandomWordAug.__init__() got an unexpected keyword argument 'candidate_words'"
     ]
    }
   ],
   "source": [
    "protected_map, protected_text = {}, text\n",
    "for i, phrase in enumerate(phrases_sorted):\n",
    "    if not phrase.strip():\n",
    "        continue\n",
    "    placeholder = f\"__PHRASE_{i}__\"\n",
    "    def _sub(m):\n",
    "        protected_map[placeholder] = m.group(0)\n",
    "        return placeholder\n",
    "    pattern = re.compile(re.escape(phrase), re.IGNORECASE)\n",
    "    protected_text = pattern.sub(_sub, protected_text)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0861b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "7c32cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple, Dict\n",
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "class YourClass:\n",
    "    def __init__(self, seed: int = 42):\n",
    "        self.seed = seed\n",
    "\n",
    "    def _apply_word_splitting(self, text: str, intensity: float) -> Tuple[str, Dict]:\n",
    "        \"\"\"\n",
    "        Split words using nlpaug's character-level SplitAug.\n",
    "        - intensity in [0,1] controls how aggressively to split characters/words.\n",
    "        - No exclusions: any token can be split.\n",
    "        \"\"\"\n",
    "        # clamp intensity\n",
    "        intensity = max(0.0, min(1.0, float(intensity)))\n",
    "\n",
    "        # Create the augmenter.\n",
    "        # For char-level augmenters, the probability parameter is aug_char_p.\n",
    "        # include_detail=True lets us retrieve what got changed (when supported).\n",
    "        aug = nac.SplitAug(\n",
    "            aug_char_p=intensity,\n",
    "            separator=\" \",\n",
    "            include_detail=True\n",
    "        )\n",
    "\n",
    "        # Set seeds for reproducibility (optional)\n",
    "        random.seed(self.seed)\n",
    "\n",
    "        # Run augmentation. With include_detail=True, most nlpaug versions return (aug_text, details).\n",
    "        try:\n",
    "            aug_text, details = aug.augment(text)\n",
    "        except Exception:\n",
    "            # Fallback for versions that return only the text\n",
    "            aug_text = aug.augment(text)\n",
    "            details = None\n",
    "\n",
    "        # Build a simple report from details when available\n",
    "        affected_words = []\n",
    "        splits = {}\n",
    "        if details:\n",
    "            # details is usually a list of change logs; each item describes one split\n",
    "            # We normalize entries defensively across versions.\n",
    "            logs = details if isinstance(details, list) else [details]\n",
    "            for log in logs:\n",
    "                # typical fields: 'orig', 'new', 'start_pos', 'end_pos', 'action'\n",
    "                orig = log.get(\"orig\") if isinstance(log, dict) else None\n",
    "                new = log.get(\"new\") if isinstance(log, dict) else None\n",
    "                action = log.get(\"action\") if isinstance(log, dict) else None\n",
    "\n",
    "                if action is None or action == \"split\":\n",
    "                    if orig and new and orig != new:\n",
    "                        affected_words.append(orig)\n",
    "                        splits[orig] = new\n",
    "\n",
    "        return aug_text, {\n",
    "            \"affected_words\": affected_words,\n",
    "            \"splits\": splits,\n",
    "            \"num_splits\": len(affected_words) if affected_words else None  # may be None if details unsupported\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb2ff03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6119d901",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nlpaug.augmenter.char' has no attribute 'SplitAug'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[206], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Try different intensities\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m intensity \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]:\n\u001b[0;32m----> 9\u001b[0m     perturbed_text, info \u001b[38;5;241m=\u001b[39m \u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_word_splitting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIntensity=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mintensity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal :\u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "Cell \u001b[0;32mIn[205], line 21\u001b[0m, in \u001b[0;36mYourClass._apply_word_splitting\u001b[0;34m(self, text, intensity)\u001b[0m\n\u001b[1;32m     16\u001b[0m intensity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(intensity)))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create the augmenter.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# For char-level augmenters, the probability parameter is aug_char_p.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# include_detail=True lets us retrieve what got changed (when supported).\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m aug \u001b[38;5;241m=\u001b[39m \u001b[43mnac\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSplitAug\u001b[49m(\n\u001b[1;32m     22\u001b[0m     aug_char_p\u001b[38;5;241m=\u001b[39mintensity,\n\u001b[1;32m     23\u001b[0m     separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     include_detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Set seeds for reproducibility (optional)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'nlpaug.augmenter.char' has no attribute 'SplitAug'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "splitter = YourClass(seed=42)\n",
    "\n",
    "text = \"The database framework handles metadata and timestamps efficiently.\"\n",
    "\n",
    "# Try different intensities\n",
    "for intensity in [0.2, 0.5, 1.0]:\n",
    "    perturbed_text, info = splitter._apply_word_splitting(text, intensity)\n",
    "    print(f\"\\nIntensity={intensity}\")\n",
    "    print(\"Original :\", text)\n",
    "    print(\"Perturbed:\", perturbed_text)\n",
    "    print(\"Details  :\", info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1395a6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action', 'Augmenter', 'CharAugmenter', 'Doc', 'KeyboardAug', 'LibraryUtil', 'Method', 'OcrAug', 'RandomCharAug', 'ReadUtil', 'Tokenizer', 'WarningCode', 'WarningException', 'WarningMessage', 'WarningName', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'absolute_import', 'char_augmenter', 'keyboard', 'nmc', 'ocr', 'os', 'random', 're', 'string']\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "print(dir(nac)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2c77a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "# --- minimal tokenizer/detokenizer (punctuation-safe) ---\n",
    "_WORD = r\"[A-Za-z]+(?:[-'][A-Za-z]+)*\"   # words incl. hyphen/apos\n",
    "_PUNCT = r\"[^\\w\\s]\"                      # single punctuation char\n",
    "TOKEN_RX = re.compile(fr\"{_WORD}|{_PUNCT}\")\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return TOKEN_RX.findall(text)\n",
    "# \n",
    "# \n",
    "# \n",
    "#  --- optional curated splits for common compounds ---\n",
    "SPLIT_MAP = {\n",
    "    \"database\": \"data base\",\n",
    "    \"dataset\": \"data set\",\n",
    "    \"framework\": \"frame work\",\n",
    "    \"workflow\": \"work flow\",\n",
    "    \"endpoint\": \"end point\",\n",
    "    \"backend\": \"back end\",\n",
    "    \"frontend\": \"front end\",\n",
    "    \"metadata\": \"meta data\",\n",
    "    \"timestamp\": \"time stamp\",\n",
    "    \"username\": \"user name\",\n",
    "    \"filename\": \"file name\",\n",
    "    \"keyword\": \"key word\",\n",
    "    \"substring\": \"sub string\",\n",
    "    \"subquery\": \"sub query\",\n",
    "}\n",
    "\n",
    "_CAMEL_RX = re.compile(r\"(?<=[a-z])(?=[A-Z])\")\n",
    "_SNAKE_RX = re.compile(r\"_+\")\n",
    "\n",
    "VOWELS = set(\"aeiouAEIOU\")\n",
    "\n",
    "\n",
    "# def _best_effort_split(word: str) -> str:\n",
    "#     \"\"\"Pick a reasonable split point near the middle if no rule applies.\"\"\"\n",
    "#     if len(word) < 3:\n",
    "#         return word  # too short to split meaningfully\n",
    "\n",
    "#     mid = len(word) // 2\n",
    "#     # Scan outward from the middle looking for a vowel/consonant boundary\n",
    "#     candidates = []\n",
    "#     for offset in range(0, mid):\n",
    "#         for i in (mid - offset, mid + offset):\n",
    "#             if 2 <= i <= len(word) - 2:\n",
    "#                 left, right = word[i - 1], word[i]\n",
    "#                 # prefer boundary where left & right differ in vowel/consonant\n",
    "#                 if (left in VOWELS) != (right in VOWELS):\n",
    "#                     candidates.append(i)\n",
    "#         if candidates:\n",
    "#             break\n",
    "#     split_idx = candidates[0] if candidates else max(2, min(len(word) - 2, mid))\n",
    "#     return word[:split_idx] + \" \" + word[split_idx:]\n",
    "\n",
    "\n",
    "\n",
    "def _best_effort_split(word: str) -> str:\n",
    "    \"\"\"Split a word at any vowel/consonant boundary, or at the middle if none found.\"\"\"\n",
    "    if len(word) < 3:\n",
    "        return word  # too short to split meaningfully\n",
    "\n",
    "    candidates = []\n",
    "    # Scan all possible split points (not just near the middle)\n",
    "    for i in range(2, len(word) - 1):\n",
    "        left, right = word[i - 1], word[i]\n",
    "        # prefer boundary where left & right differ in vowel/consonant\n",
    "        if (left in VOWELS) != (right in VOWELS):\n",
    "            candidates.append(i)\n",
    "    # Pick the first found boundary, or fallback to middle-ish\n",
    "    split_idx = candidates[0] if candidates else max(2, min(len(word) - 2, len(word) // 2))\n",
    "    return word[:split_idx] + \" \" + word[split_idx:]\n",
    "\n",
    "\n",
    "\n",
    "def _split_token(tok: str) -> str:\n",
    "    low = tok.lower()\n",
    "    # 1) curated map\n",
    "    if low in SPLIT_MAP:\n",
    "        out = SPLIT_MAP[low]\n",
    "        # preserve capitalization of the first piece if original was capitalized\n",
    "        parts = out.split()\n",
    "        if tok[0].isupper():\n",
    "            parts[0] = parts[0].capitalize()\n",
    "        return \" \".join(parts)\n",
    "    # 2) camelCase\n",
    "    if _CAMEL_RX.search(tok):\n",
    "        return _CAMEL_RX.sub(\" \", tok)\n",
    "    # 3) snake_case\n",
    "    if _SNAKE_RX.search(tok):\n",
    "        return _SNAKE_RX.sub(\" \", tok)\n",
    "    # 4) best-effort near middle\n",
    "    return _best_effort_split(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c9899fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ma chinelearning'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_split_token(\"machinelearning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387b121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rankingSHAP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
